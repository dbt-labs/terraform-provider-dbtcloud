---
page_title: "{{.Name}} {{.Type}} - {{.RenderedProviderName}}"
subcategory: ""
description: |-
{{ .Description | plainmarkdown | trimspace | prefixlines "  " }}
---

# {{.Name}} ({{.Type}})

~> In October 2023, CI improvements have been rolled out to dbt Cloud with minor impacts to some jobs:  [more info](https://docs.getdbt.com/docs/dbt-versions/release-notes/june-2023/ci-updates-phase1-rn). 
<br/>
<br/>
Those improvements include modifications to deferral which was historically set at the job level and will now be set at the environment level. 
Deferral can still be set to "self" by setting `self_deferring` to `true` but with the new approach, deferral to other runs need to be done with `deferring_environment_id` instead of `deferring_job_id`.

~> New with 0.3.1, `triggers` now accepts a `on_merge` value to trigger jobs when code is merged in git. If `on_merge` is `true` all other triggers need to be `false`.
<br/>
<br/>
For now, it is not a mandatory field, but it will be in a future version. Please add `on_merge` in your config or modules. 

## Example Usage

{{ tffile (printf "%s%s%s" "examples/resources/" .Name "/resource.tf") }}


## Advanced Usage for setting up deferral

To use deferral in dbt Cloud, some jobs need to run successfully so that we can levereage the manifest file generated.

It is possible to have those jobs run automatically based on their schedule, or to run them manually, but in the scenarion where many dbt Cloud projects are created, it might be easier to automate this process.

### For Continuous Integration

In the case of Continuous Integration, our CI job needs to defer to the Production environment.
So, we need to have a successful run in the Production environment before the CI process can execute as expected.

The example below shows how the Terraform config can be updated to automatically trigger a run of the job in the Production environment, leveraging the `local-exec` provisioner and `curl` to trigger the run.

```tf
# a periodic job, but we trigger it once with `dbt parse` as soon as it is created so we can defer to the environment it is in
# to do so, we use a local-exec provisioner, just make sure that the machine running Terraform has curl installed
resource "dbtcloud_job" "daily_job" {
  environment_id = dbtcloud_environment.prod_environment.environment_id
  execute_steps = [
    "dbt build"
  ]
  generate_docs        = true
  name                 = "Daily job"
  num_threads          = 64
  project_id           = dbtcloud_project.dbt_project.id
  run_generate_sources = true
  target_name          = "default"
  triggers = {
    "github_webhook" : false
    "git_provider_webhook" : false
    "schedule" : true
    "on_merge" : false
  }

  schedule_days  = [0, 1, 2, 3, 4, 5, 6]
  schedule_type  = "days_of_week"
  schedule_hours = [0]

  provisioner "local-exec" {
    command = <<-EOT
      response=$(curl -s -L -o /dev/null -w "%%{http_code}" -X POST \
        -H 'Authorization: Bearer ${var.dbt_token}' \
        -H 'Content-Type: application/json' \
        -d '{"cause": "Generate manifest", "steps_override": ["dbt parse"]}' \
        ${var.dbt_host_url}/v2/accounts/${var.dbt_account_id}/jobs/${self.id}/run/)
      
      if [ "$response" -ge 200 ] && [ "$response" -lt 300 ]; then
        echo "Success: HTTP status $response"
        exit 0
      else
        echo "Failure: HTTP status $response"
        exit 1
      fi
    EOT
  }
}
```

### For allowing source freshness deferral

In the case that deferral is required so that we can use [the `source_status:fresher+` selector](https://docs.getdbt.com/docs/build/sources#build-models-based-on-source-freshness), 
the process is more complicated as the job will be self deferring.

An example can be found [in this GitHub issue](https://github.com/dbt-labs/terraform-provider-dbtcloud/issues/360#issuecomment-2779336961).

{{ .SchemaMarkdown | trimspace }}

## Import

Import is supported using the following syntax:

{{ codefile "shell" (printf "%s%s%s" "examples/resources/" .Name "/import.sh") }}